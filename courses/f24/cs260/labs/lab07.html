
<link href='../markdown.css' rel='stylesheet'></link>

<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
  });
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<!-- - - - - - - - - - - - - - - - - - - - - - - -->

<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>lab07</title>
  <style>
    html {
      line-height: 1.7;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 40em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin-top: 1.7em;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.7em;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1.7em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1.7em 0 1.7em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      font-style: italic;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      background-color: #f0f0f0;
      font-size: 85%;
      margin: 0;
      padding: .2em .4em;
    }
    pre {
      line-height: 1.5em;
      padding: 1em;
      background-color: #f0f0f0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin-top: 1.7em;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
    }
    th, td {
      border-bottom: 1px solid lightgray;
      padding: 1em 3em 1em 0;
    }
    header {
      margin-bottom: 6em;
      text-align: center;
    }
    nav a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="cs260-lab-7-logistic-regression-and-visualization">CS260 Lab 7: Logistic Regression and Visualization</h1>
<h3><a href="https://classroom.github.com/a/Y5rhIad5">GitHub Classroom Assignment Link</a></h3>
<hr />
<h2 id="overview">Overview</h2>
<p>The goals of this week’s lab:</p>
<ul>
<li>Understand and implement Logistic Regression</li>
<li>Apply Logistic Regression to real-world datasets</li>
<li>Visualize data and models in a meaningful way</li>
</ul>
<p>In this lab we will be analyzing two datasets:</p>
<ul>
<li><p><a href="https://en.wikipedia.org/wiki/Phoneme">phoneme</a> dataset: The task of phoneme classification is to predict the label of a phoneme articulation given its corresponding voice signal. In this problem, we have voice signals from two different phonemes: “aa” (as in “bot”) and “ae” (as in “bat”). They are from an acoustic-phonetic corpus called <a href="https://catalog.ldc.upenn.edu/LDC93S1">TIMIT</a>. Phoneme classification is an important part of a speech-to-text system.</p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster">Challenger</a> dataset: on 1/28/86 the Challenger space shuttle exploded due to an O-ring failure. This dataset shows the results of previous O-ring experiments.</p></li>
</ul>
<p><br></p>
<hr />
<h2 id="getting-started">Getting Started</h2>
<p>Accept your Lab 7 repository on Github classroom. You should have the following files:</p>
<ul>
<li><strong>data</strong> - directory containing train/test data sets. You may add additional data sets here.</li>
<li><code>run_LR.py</code> - your main program executable for Logistic Regression.</li>
<li><code>LogisticRegression.py</code> - file for the <code>LogisticRegression</code> class (and/or functions).</li>
<li><code>README.md</code> - for analysis questions and lab feedback.</li>
</ul>
<p><br></p>
<hr />
<h2 id="usage-and-io">Usage and I/O</h2>
<h3 id="usage">Usage</h3>
<p>Your programs should take in the same command-line arguments as Lab 4 (feel free to reuse the argument parsing code), plus a parameter for the learning rate <code>alpha</code>. For example, to run Logistic Regression on the phoneme example:</p>
<pre><code>python3 run_LR.py input/phoneme_train.csv input/phoneme_test.csv 0.02</code></pre>
<h3 id="program-inputs">Program Inputs</h3>
<p>To simplify preprocessing, you may assume the following:</p>
<ul>
<li><p>The datasets will have continuous features, and binary labels (0, 1). The train and test datasets will be in CSV form (comma separated values), with the label as the last column.</p></li>
<li><p>I would recommend creating a function to parse the CSV file format. Use the method <code>split(',')</code> to split the line (type string) into a list of strings, which can be parsed further.</p></li>
<li><p>Make sure the user enters a positive <code>alpha</code> value (it should be type float)</p></li>
</ul>
<h3 id="program-outputs">Program Outputs</h3>
<ul>
<li><p>Your program should <em>print</em> a confusion matrix and accuracy of the result (see examples below).</p></li>
<li><p>The design of your solution is largely up to you. You don’t necessarily need to have a class, but you should use good top-down design principles so that your code is readable. Having functions for the cost, for SGD, for the logistic function, etc is a good idea.</p></li>
</ul>
<hr />
<h2 id="logistic-regression">Logistic Regression</h2>
<p>You will implement the logistic regression task discussed in class for <em>binary classification</em>.</p>
<h3 id="model">Model</h3>
<p>For now you should assume that the features will be continuous and the response will be a discrete, binary output. In the case of binary labels, the probability of a positive prediction is:</p>
<center>
<img src="figs/log_reg.png" width="500">
</center>
<p><br></p>
<p>To model the intercept term, we will include a bias term. In practice, this is equivalent to adding a feature that is always “on”. For each instance in training and testing, append a <strong>1</strong> to the feature vector. Ensure that your logistic regression model has a weight for this feature that it can now learn in the same way it learns all other weights.</p>
<h3 id="training">Training</h3>
<p>To learn the weights, you will apply stochastic gradient descent as discussed in class until the cost function does not change in value (very much) for a given iteration. As a reminder, our cost function is the <em>negative log</em> of the likelihood function, which is:</p>
<center>
<img src="figs/likelihood.png" width="500">
</center>
<p><br></p>
<p>Our goal is to minimize the cost using SGD. We will use the same idea as in Lab 3 for linear regression. Pseudocode:</p>
<pre><code>initialize weights to 0&#39;s
while not converged:
    shuffle the training examples
    for each training example xi:
        calculate derivative of cost with respect to xi
        weights = weights - alpha*derivative
    compute and store current cost</code></pre>
<p>The SGD updates for <span class="math inline"><em>w</em></span> term are:</p>
<center>
<img src="figs/sgd1.png" width="500">
</center>
<p><br></p>
<p>The hyper-parameter <em>alpha</em> (learning rate) should be sent as a parameter to your SGD and used in training. A few notes for the above:</p>
<ul>
<li>In Lab 3 we did not actually shuffle the data points to make gradient descent truly “stochastic”, but for this lab you should (also make sure y is shuffled along with X!). You may use:</li>
</ul>
<pre><code>numpy.random.shuffle(A)</code></pre>
<p>which by default shuffles along the first axis.</p>
<ul>
<li><p>The stopping criteria should be a) a maximum of some number of iterations (your choice) OR b) the cost has changed by less some number between two iterations (your choice).</p></li>
<li><p>Many of the operations above are on vectors. I recommend using <code>numpy</code> features such as dot product to make the code simple.</p></li>
<li><p>Try to choose hyper-parameters that maximize the testing accuracy.</p></li>
</ul>
<h3 id="prediction">Prediction</h3>
<p>For binary prediction, apply the equation:</p>
<center>
<img src="figs/log_reg.png" width="500">
</center>
<p><br></p>
<p>and output the more likely binary outcome. You may use the Python <code>math.exp</code> function to make the calculation in the denominator. Alternatively, you may use the weights directly as a decision boundary as discussed in class.</p>
<p>The choice of hyper-parameters will affect the results, but you should be able to obtain <strong>at least 90% accuracy on the testing data</strong>. Make sure to print a confusion matrix.</p>
<h3 id="example-output">Example Output</h3>
<p>Here is an example run for testing purposes. You should be able to get a better result than this, but this will check if your algorithm is working. The details are:</p>
<ul>
<li>No shuffling of the training data points (which you should do for the final result!)</li>
<li><code>alpha</code> = 0.02</li>
<li>stopping criteria: looking for changes in the cost <span class="math inline"><em>J</em>(<em>w</em>)</span></li>
<li><code>epsilon</code> = 1e-4 (see if cost has changed by this amount)</li>
</ul>
<pre><code>$ python3 run_LR.py input/phoneme_train.csv input/phoneme_test.csv 0.02
Accuracy: 0.875000 (70 out of 80 correct)

   prediction
      0  1
    ------
  0| 36  4
  1|  6 34</code></pre>
<p><a href="out/phoneme.labels">Labels</a> and <a href="out/phoneme.debug">Debugging Output</a></p>
<hr />
<h2 id="visualization">Visualization</h2>
<p>For just the Challenger dataset, create a visualization of both:</p>
<ul>
<li>the data (both training and testing, even though the “test” data here is just one datapoint)</li>
<li>the model (i.e. the final logistic function)</li>
</ul>
<p>Make sure to think about visualization practices discussed in class including:</p>
<ul>
<li>choice of colors</li>
<li>legend or trying to incorporate the legend into the figure</li>
<li>overplotting</li>
<li>title/axis labels etc</li>
</ul>
<p>Submit your figure as part of your github repo using the filename <code>challenger.pdf</code>.</p>
<hr />
<h2 id="analysis">Analysis</h2>
<p>(See README.md to answer these questions)</p>
<ol type="1">
<li><p>So far, we have run logistic regression and Naive Bayes on different types of datasets. Why is that? Discuss the types of features and labels that we worked with in each case.</p></li>
<li><p>Regarding the Challenger data specifically, what was your final probability of an accident after fitting the model? Is this what you expected? Based on this value, would you have recommended the Challenger be launched on 1/28/86? What data and/or modeling might have helped increase the confidence of our prediction?</p></li>
</ol>
<hr />
<h2 id="acknowledgements">Acknowledgements</h2>
<p><em>Credit for this lab: Materials by Sara Mathieson, modified from material created by Ameet Soni. Phoneme dataset and information created by Jessica Wu. Challenger dataset and visualization ideas from Alvin Grissom.</em></p>
</body>
</html>
